# âš¡ Databricks Cost-Effective Optimization Cheat Sheet

> **Goal:** Choose the smallest possible cluster that finishes the job within SLA.

---

## ðŸŽ¯ Decision Order (Most Important)

```
Workload Type â†’ Data Size â†’ SLA â†’ Cost
```

**Cluster sizing depends more on WORKLOAD TYPE than data size.**

---

## ðŸ“Š Quick Cluster Sizing Guide

### ðŸ”„ Batch / ETL (Spark Jobs)

| Data Size | Recommended Cluster | Notes |
|-----------|-------------------|-------|
| â‰¤ 10 GB | Single Node (4â€“8 GB RAM) | Start here |
| 10â€“50 GB | 2â€“4 workers, 4â€“8 cores | Most common |
| 50â€“200 GB | 4â€“8 workers, 8â€“16 cores | Add autoscaling |
| 200 GBâ€“1 TB | 8â€“16 workers, autoscaling | Tune shuffle |
| > 1 TB | 16+ workers | Optimize partitions |

ðŸ’¡ **Most ETL jobs are memory-bound, not CPU-bound.**

---

### ðŸ“ˆ SQL / BI Analytics

| Data Size | Cluster | Settings |
|-----------|---------|----------|
| â‰¤ 50 GB | 2â€“4 workers | Photon ON |
| 50â€“500 GB | 4â€“8 workers | Photon ON |
| > 500 GB | 8â€“16 workers | Photon ON + Cache |

âœ… **Prefer Photon ON**  
âœ… **Fewer, larger nodes**  
âœ… **Cache only frequently queried tables**

---

### ðŸ” Streaming (Structured Streaming)

| Throughput | Cluster | Notes |
|------------|---------|-------|
| Low | 2 workers | Stable cluster |
| Medium | 4â€“6 workers | Avoid autoscaling |
| High | 8â€“12 workers | Scale horizontally |

âš ï¸ **Small, stable clusters**  
âš ï¸ **Avoid autoscaling unless traffic is unpredictable**

---

### ðŸ¤– ML Training

| Workload | Cluster | Best Practice |
|----------|---------|---------------|
| Small models | 2â€“4 workers | Job clusters only |
| Large models | 4â€“8 memory-optimized | Separate training/inference |
| GPU | GPU instances | Use only when proven necessary |

âœ… **Separate training and inference clusters**  
âœ… **Use job clusters only**  
âœ… **Prefer fewer, powerful nodes**

---

## ðŸ”€ Autoscaling Rules

### âœ… Use Autoscaling When:
- Workload is spiky
- Data size varies daily
- Unpredictable traffic patterns

### âŒ Avoid Autoscaling When:
- Small, predictable batch jobs
- Streaming with stable throughput
- Fixed-size daily ETL

**Best Practice:** `min = expected load, max = 2Ã— expected load`

---

## ðŸ’¸ Cost Killers (Avoid These)

| âŒ Mistake | Impact |
|------------|--------|
| Always-on all-purpose clusters | High idle costs |
| Over-partitioning | Too many small tasks |
| Too many small workers | Inefficient resource use |
| ML jobs on ETL clusters | Wrong instance types |
| Caching everything | Unnecessary memory usage |

---

## ðŸ’° Cost-Saving Power Moves

| âœ… Action | Benefit |
|-----------|---------|
| Use Job Clusters (auto-terminate) | No idle costs |
| Turn Photon ON for SQL/ETL | 2â€“5Ã— faster, lower cost |
| Use Spot instances for batch | 50â€“90% savings |
| Repartition to 2â€“4Ã— total cores | Optimal parallelism |
| Enable auto-termination (10â€“30 min) | Prevents idle waste |

---

## ðŸ† Golden Rule

```
Start Small â†’ Measure â†’ Scale Only If Needed
```

**Most Databricks jobs are over-provisioned by default.**

---

## ðŸ“‹ Quick Checklist

- [ ] Identified workload type (Batch/SQL/Streaming/ML)
- [ ] Started with smallest recommended cluster
- [ ] Enabled Photon for SQL/ETL workloads
- [ ] Configured auto-termination
- [ ] Used job clusters (not all-purpose)
- [ ] Set autoscaling only if needed
- [ ] Repartitioned to 2â€“4Ã— cores
- [ ] Measured performance before scaling up

---

## ðŸŽ“ Pro Tips

1. **Memory > CPU**: Most Spark jobs are memory-bound
2. **Horizontal > Vertical**: Scale out, not up
3. **Job Clusters**: Always use for scheduled jobs
4. **Photon**: Default ON for SQL/ETL
5. **Spot Instances**: Use for fault-tolerant batch jobs
6. **Partitioning**: 128MBâ€“200MB per partition ideal
7. **Caching**: Only cache what you query repeatedly

---

**Remember:** Start small, measure, then scale. Most clusters are over-provisioned! ðŸš€
